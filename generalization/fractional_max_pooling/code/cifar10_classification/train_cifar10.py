# -*- coding: utf-8 -*-
"""train_cifar10.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UIpHm9yDIq_P_YojYAJ4JF4OdJxyk6FK
"""

# from google.colab import drive
# drive.mount('/content/drive', force_remount=True)
# import sys
# import os
# py_file_location = "/content/drive/My Drive/generalization/"
# sys.path.append(os.path.abspath(py_file_location))
import argparse

# Import Necessary Packages
from cifar10_models import *
from dataset import getDatset
import numpy as np
from matplotlib import pyplot as plt 
import torch 
import torch.nn as nn
# from tqdm.notebook import tqdm_notebook as tqdm
from tqdm import tqdm
import torch.backends.cudnn as cudnn
from  torch.cuda.amp import autocast, GradScaler
from copy import deepcopy
import os 
import sys

@torch.no_grad()
def accuracy(outputs, labels):
    _, preds = torch.max(outputs, dim=1)
    return torch.tensor(torch.sum(preds == labels).item() / len(preds))

def train(model,model_name, criterion, loss_val,  optimizer, dataloader, epochs=200, resume=False, save=False, path=None, device='cpu'):

    model = model.to(device)


    criterion = criterion.to(device)
    loss_val = loss_val.to(device)
    scaler = GradScaler()
    
    if device == 'cuda':
        model = torch.nn.DataParallel(model)
        cudnn.benchmark = True 
    new_path = model_name + ".pth.tar"
    if resume:
        print("\n Resume Training from Checkpoint \n ")

        assert path is not None, "Enter Proper Path for loading stored weights"
        
        checkpoint = torch.load(os.path.join(path, new_path))

        model.load_state_dict(deepcopy(checkpoint['model']))
        optimizer.load_state_dict(deepcopy(checkpoint['optimizer']))
        # scheduler.load_state_dict(deepcopy(checkpoint['scheduler']))
        epoch_start = deepcopy(checkpoint['epoch'])
        loss_list = deepcopy(checkpoint['train_loss'])
        train_acc = deepcopy(checkpoint['train_acc'])
        val_loss_list = deepcopy(checkpoint['valid_loss'])
        val_acc = deepcopy(checkpoint['valid_acc'])

    else:
        epoch_start = 0
        loss_list = []
        train_acc = []
        val_loss_list = []
        val_acc = []
    
    for epoch in range(epoch_start, epochs):

        epoch_loss = 0
        epoch_acc = 0
        epoch_val_loss = 0
        epoch_val_acc = 0

        model.train()
        
        with tqdm(dataloader[0], unit='batch', total=len(dataloader[0])) as train_loader:
            train_loader.set_description(f"Epoch {epoch+1:4d}/{epochs}")

            for idx, (images, labels) in enumerate(train_loader):
                images, labels = images.to(device), labels.to(device)
                
                with autocast():
                  # Forward Propagation
                  output = model(images)
                  # Computing Loss
                  loss = criterion(output, labels)

                  optimizer.zero_grad(set_to_none=True)

                  # Backward Propagation

                  scaler.scale(loss).backward()
                  epoch_loss += loss.item() / len(dataloader[0])
                  # Update Weights
                  scaler.step(optimizer)
                  # optimizer.step()
                  scaler.update()

                  epoch_acc += accuracy(output, labels).numpy() / len(dataloader[0])

        loss_list.append(deepcopy(epoch_loss))
        train_acc.append(deepcopy(epoch_acc))
        
        with torch.no_grad():
            model.eval()

            for idx , (images, labels) in enumerate(dataloader[1]):
                images, labels = images.to(device), labels.to(device)
                output = model(images)

                loss_v = loss_val(output, labels)

                epoch_val_loss += loss_v.item() / len(dataloader[1])
                
                epoch_val_acc += accuracy(output, labels).numpy() / len(dataloader[1])

            val_loss_list.append(deepcopy(epoch_val_loss))
            val_acc.append(deepcopy(epoch_val_acc))

        print("                 Training loss: %.4f  Training acc: %.4f  Validation loss: %.4f  Validation acc: %.4f" % (float(loss_list[-1]),float(train_acc[-1]),float(val_loss_list[-1]), float(val_acc[-1]),))

        torch.save({"epoch": epoch + 1,
                    "model": model.state_dict(),
                    "optimizer": optimizer.state_dict(),
                    "train_loss": loss_list,
                    "train_acc": train_acc,
                    "valid_loss": val_loss_list,
                    "valid_acc": val_acc},os.path.join(path, new_path))


def main():
    parser = argparse.ArgumentParser(description='Training Model for Cifar10 Classification')
    parser.add_argument('--model', type=str, required=True)
    parser.add_argument('--data',type=str, required=True)
    parser.add_argument('--store' ,type=str, required=True)

    parser.add_argument('--resume' ,type=bool, default=False)
    parser.add_argument('--save' ,type=bool, default=False)
    parser.add_argument('--epochs', type=int, default=200)

    args = parser.parse_args()
    dataloader = getDatset(args.data)

    if args.model == "FCN":
        hn_ = [256, 128]
        c_ = 10
        model = FullyConnectedNet(hn_, c_, 0.5, 3072)
        

    if args.model == "VGG":
        k_list = [64, 128, 256, 512, 512]
        l_list = [2, 2, 3, 3, 3]
        h_list = [256, 128]
        classes = 10 
        dp_list = [0.4, 0.5]
        model = VGG(in_channels=3, out_channel_list=k_list, layers=l_list, h_list=h_list, classes=classes, dp_list=dp_list)

    if args.model == "FMP":

        k_list = [32, 64, 96, 128, 160, 192]
        d_list = [0.3, 0.33, 0.37, 0.4, 0.45, 0.5]
        Classes = 10
        fraction = 1/np.sqrt(2)
        model = FMPNet(3, k_list, d_list, fraction,Classes)

    if args.model == "M2D":

        k_list = [32, 64, 96, 128, 160, 192]
        d_list = [0.3, 0.33, 0.37, 0.4, 0.45, 0.5]
        Classes = 10
        model = MaxPool2DNet(3, k_list, d_list, Classes)

    model = model.to("cuda")
    # summary(model, input_size=(3, 32, 32))
    # print(args.model)
    # print(args.data)
    # print(args.store)
    # print(args.epochs)
    # print(args.resume)
    # print(args.save)
    loss_train = nn.CrossEntropyLoss()
    loss_val = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001, weight_decay=1e-4)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    train(model,args.model, loss_train, loss_val, optimizer, dataloader, epochs=args.epochs, resume=args.resume, save=args.save, device=device, path=args.store)
    print(args.model)

if __name__ == "__main__":
    main()

